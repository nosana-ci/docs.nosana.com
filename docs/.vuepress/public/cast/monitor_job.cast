{"version":2,"width":100,"height":24,"timestamp":1741693610,"env":{"TERM":"xterm-256color","SHELL":"/bin/bash"},"theme":{"fg":"#4c4f69","bg":"#eff1f5","palette":"#bcc0cc:#d20f39:#40a02b:#df8e1d:#1e66f5:#ea76cb:#179299:#5c5f77:#acb0be:#d20f39:#40a02b:#df8e1d:#1e66f5:#ea76cb:#179299:#6c6f85"}}
[0.306805, "o", "\u001b[?2004h"]
[0.306812, "o", "\u001b]0;nosana@nos-os: ~\u0007\u001b[01;32mnosana@nos-os\u001b[00m:\u001b[01;34m~\u001b[00m$ "]
[2.63713, "o", "\u001b[7mnpx @nosana/cli job get B7owLMjUup9znGX6BHVoUm2fwo2dgt8frG6dTE6pMU9y --wait\u001b[27m\r\n\r"]
[3.627773, "o", "\u001b[A\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[Cnpx @nosana/cli job get B7owLMjUup9znGX6BHVoUm2fwo2dgt8frG6dTE6pMU9y --wait\r\n\r\u001b[A\r\n\u001b[?2004l\r"]
[3.883726, "o", "\u001b[1G"]
[3.8839, "o", "\u001b[0K"]
[3.883915, "o", "⠙"]
[3.964521, "o", "\u001b[1G\u001b[0K⠹"]
[4.045406, "o", "\u001b[1G\u001b[0K"]
[4.045413, "o", "⠸"]
[4.125622, "o", "\u001b[1G"]
[4.125629, "o", "\u001b[0K"]
[4.12563, "o", "⠼"]
[4.206225, "o", "\u001b[1G\u001b[0K"]
[4.206233, "o", "⠴"]
[4.286019, "o", "\u001b[1G\u001b[0K⠦"]
[4.366463, "o", "\u001b[1G\u001b[0K⠧"]
[4.44709, "o", "\u001b[1G\u001b[0K⠇"]
[4.531594, "o", "\u001b[1G\u001b[0K⠏"]
[4.579169, "o", "\u001b[1G\u001b[0K"]
[5.742819, "o", "  _   _                             \r\n | \\ | | ___  ___  __ _ _ __   __ _ \r\n |  \\| |/ _ \\/ __|/ _` | '_ \\ / _` |\r\n | |\\  | (_) \\__ \\ (_| | | | | (_| |\r\n |_| \\_|\\___/|___/\\__,_|_| |_|\\__,_|\r\n                                    \r\n"]
[5.74351, "o", "Reading keypair from \u001b[1;36m/home/user/.nosana/nosana_key.json\u001b[0m\r\n\r\n"]
[6.614953, "o", "Network:\t\u001b[1;32mmainnet\u001b[0m\r\n"]
[7.66841, "o", "Wallet:\t\t\u001b[1;32mCTYw7JqNeh92BLFCJ5pR9HbpZHCsQPxtV2mZdD7WY2bD\u001b[0m\r\n"]
[7.668453, "o", "SOL balance:\t\u001b[1;32m0.289395494 SOL\u001b[0m\r\n"]
[7.668465, "o", "NOS balance:\t\u001b[1;32m0.609734 NOS\u001b[0m\r\n"]
[7.855791, "o", "retrieving job...\r\n"]
[8.445955, "o", "\u001b[1A"]
[8.445967, "o", "\u001b[0K"]
[8.446035, "o", "Job:\t\t\u001b[1;34mhttps://dashboard.nosana.com/jobs/B7owLMjUup9znGX6BHVoUm2fwo2dgt8frG6dTE6pMU9y\u001b[0m\r\n"]
[8.44619, "o", "Market:\t\t\u001b[1;34mhttps://dashboard.nosana.com/markets/CA5pMpqkYFKtme7K31pNB1s62X2SdhEv1nN9RdxKCpuQ\u001b[0m\r\n"]
[8.446258, "o", "Status:\t\t\u001b[1;36mRUNNING\u001b[0m\r\n"]
[8.447813, "o", "\u001b[?25l"]
[8.447966, "o", "\u001b[1G\u001b[1G"]
[8.448041, "o", "\u001b[36m⠋\u001b[39m \u001b[36mWaiting for job to complete\u001b[39m"]
[8.527852, "o", "\u001b[1G\u001b[0K"]
[8.52789, "o", "\u001b[36m⠙\u001b[39m \u001b[36mWaiting for job to complete\u001b[39m"]
[8.608258, "o", "\u001b[1G\u001b[0K\u001b[36m⠹\u001b[39m \u001b[36mWaiting for job to complete\u001b[39m"]
[8.687986, "o", "\u001b[1G\u001b[0K\u001b[36m⠸\u001b[39m \u001b[36mWaiting for job to complete\u001b[39m"]
[8.768443, "o", "\u001b[1G\u001b[0K"]
[8.76845, "o", "\u001b[36m⠼\u001b[39m \u001b[36mWaiting for job to complete\u001b[39m"]
[8.81364, "o", "\u001b[1G\u001b[0K"]
[8.813646, "o", "\u001b[?25h"]
[8.813725, "o", "\u001b[32m✔\u001b[39m \u001b[36mWaiting for job to complete\u001b[39m\r\n"]
[8.813733, "o", "\u001b[1A\u001b[0K"]
[8.813742, "o", "\u001b[1A\u001b[0K"]
[8.813792, "o", "Status:\t\t\u001b[1;36mRUNNING\u001b[0m\r\n"]
[8.81383, "o", "Node:\t\t\u001b[1;34mhttps://dashboard.nosana.com/nodes/kUtyYeECw8g6nVbrMmHwGHfseaEQSddFRnp8iUpxXHH\u001b[0m\r\n"]
[12.684793, "o", "\u001b[32mNode has found job \u001b[1mB7owLMjUup9znGX6BHVoUm2fwo2dgt8frG6dTE6pMU9y\u001b[22m\u001b[39m\r\n"]
[12.685534, "o", "\u001b[?25l"]
[12.685583, "o", "\u001b[1G\u001b[1G\u001b[36m⠋\u001b[39m \u001b[36mNode is claiming job \u001b[1mB7owLMjUup9znGX6BHVoUm2fwo2dgt8frG6dTE6pMU9y\u001b[22m\u001b[39m"]
[12.685721, "o", "\u001b[1G"]
[12.685724, "o", "\u001b[0K"]
[12.685738, "o", "\u001b[?25h"]
[12.68576, "o", "\u001b[32m✔\u001b[39m \u001b[32mNode has claimed job \u001b[1mB7owLMjUup9znGX6BHVoUm2fwo2dgt8frG6dTE6pMU9y\u001b[22m\u001b[39m\r\n"]
[12.685812, "o", "\u001b[36mJob \u001b[1mB7owLMjUup9znGX6BHVoUm2fwo2dgt8frG6dTE6pMU9y\u001b[22m is starting\u001b[39m\r\n"]
[12.686271, "o", "\u001b[?25l"]
[12.686299, "o", "\u001b[1G\u001b[1G"]
[12.68632, "o", "\u001b[36m⠋\u001b[39m \u001b[36mFlow \u001b[1mB7owLMjUup9znGX6BHVoUm2fwo2dgt8frG6dTE6pMU9y\u001b[22m is intializing\u001b[39m"]
[12.686369, "o", "\u001b[1G"]
[12.686372, "o", "\u001b[0K"]
[12.686379, "o", "\u001b[?25h"]
[12.686395, "o", "\u001b[32m✔\u001b[39m \u001b[32mFlow \u001b[1mB7owLMjUup9znGX6BHVoUm2fwo2dgt8frG6dTE6pMU9y\u001b[22m is initialized\u001b[39m\r\n"]
[12.686724, "o", "\u001b[?25l\u001b[1G\u001b[1G"]
[12.686726, "o", "\u001b[36m⠋\u001b[39m \u001b[36mResolving job definition\u001b[39m"]
[12.68677, "o", "\u001b[1G\u001b[0K"]
[12.686776, "o", "\u001b[?25h"]
[12.686794, "o", "\u001b[32m✔\u001b[39m \u001b[32mJob definition retrived successfully\u001b[39m\r\n"]
[12.687096, "o", "\u001b[?25l"]
[12.687123, "o", "\u001b[1G\u001b[1G"]
[12.687132, "o", "\u001b[36m⠋\u001b[39m \u001b[36mValidating job definition\u001b[39m"]
[12.687309, "o", "\u001b[1G\u001b[0K"]
[12.687317, "o", "\u001b[?25h"]
[12.687337, "o", "\u001b[32m✔\u001b[39m \u001b[32mJob definition validated successfully\u001b[39m\r\n"]
[12.68784, "o", "\u001b[?25l"]
[12.687856, "o", "\u001b[1G\u001b[1G"]
[12.687879, "o", "\u001b[36m⠋\u001b[39m \u001b[36mFlow \u001b[1mB7owLMjUup9znGX6BHVoUm2fwo2dgt8frG6dTE6pMU9y\u001b[22m is starting\u001b[39m"]
[12.687924, "o", "\u001b[1G\u001b[0K"]
[12.687932, "o", "\u001b[?25h"]
[12.687949, "o", "\u001b[32m✔\u001b[39m \u001b[32mFlow \u001b[1mB7owLMjUup9znGX6BHVoUm2fwo2dgt8frG6dTE6pMU9y\u001b[22m started\u001b[39m\r\n"]
[12.687989, "o", "\u001b[32mJob \u001b[1mB7owLMjUup9znGX6BHVoUm2fwo2dgt8frG6dTE6pMU9y\u001b[22m started successfully\u001b[39m\r\n"]
[12.68801, "o", "\u001b[36mFlow \u001b[1mB7owLMjUup9znGX6BHVoUm2fwo2dgt8frG6dTE6pMU9y\u001b[22m is running\u001b[39m\r\n"]
[12.688026, "o", "\u001b[36mRunning action \u001b[1mcontainer/run\u001b[22m, for flow \u001b[1mB7owLMjUup9znGX6BHVoUm2fwo2dgt8frG6dTE6pMU9y\u001b[22m operation \u001b[1mqwen1.5b\u001b[22m\u001b[39m\r\n"]
[12.688414, "o", "\u001b[?25l"]
[12.68844, "o", "\u001b[1G"]
[12.688446, "o", "\u001b[1G"]
[12.688455, "o", "\u001b[36m⠋\u001b[39m \u001b[36mPulling image \u001b[1mdocker.io/vllm/vllm-openai:v0.7.2\u001b[22m\u001b[39m"]
[12.688507, "o", "\u001b[1G"]
[12.688513, "o", "\u001b[0K"]
[12.68852, "o", "\u001b[?25h"]
[12.688559, "o", "\u001b[36mpulling image docker.io/vllm/vllm-openai:v0.7.2\u001b[39m\r\n"]
[12.689062, "o", "\u001b[?25l"]
[12.689088, "o", "\u001b[?7l"]
[12.689618, "o", "\u001b[?25h\u001b[?7h"]
[12.689646, "o", "\u001b[1G"]
[12.689674, "o", "\u001b[0J"]
[12.68971, "o", "\u001b[32mPulled image \u001b[1mdocker.io/vllm/vllm-openai:v0.7.2\u001b[22m\u001b[39m\r\n"]
[12.690065, "o", "\u001b[?25l"]
[12.690082, "o", "\u001b[1G\u001b[1G"]
[12.690091, "o", "\u001b[36m⠋\u001b[39m \u001b[36mCreating network \u001b[1mB7owLMjUup9znGX6BHVoUm2fwo2dgt8frG6dTE6pMU9y-qwen1.5b\u001b[22m\u001b[39m"]
[12.690163, "o", "\u001b[1G\u001b[0K"]
[12.690176, "o", "\u001b[?25h"]
[12.690188, "o", "\u001b[32m✔\u001b[39m \u001b[32mCreated network \u001b[1mB7owLMjUup9znGX6BHVoUm2fwo2dgt8frG6dTE6pMU9y-qwen1.5b\u001b[22m\u001b[39m\r\n"]
[12.690542, "o", "\u001b[?25l"]
[12.690557, "o", "\u001b[1G\u001b[1G"]
[12.690577, "o", "\u001b[36m⠋\u001b[39m \u001b[36mPulling image \u001b[1mregistry.hub.docker.com/nosana/frpc:0.1.0\u001b[22m\u001b[39m"]
[12.690681, "o", "\u001b[1G"]
[12.690684, "o", "\u001b[0K"]
[12.690696, "o", "\u001b[?25h"]
[12.690728, "o", "\u001b[32m✔\u001b[39m \u001b[32mPulled image \u001b[1mregistry.hub.docker.com/nosana/frpc:0.1.0\u001b[22m\u001b[39m\r\n"]
[12.691139, "o", "\u001b[?25l"]
[12.691162, "o", "\u001b[1G\u001b[1G"]
[12.691175, "o", "\u001b[36m⠋\u001b[39m \u001b[36mStarting container \u001b[1mregistry.hub.docker.com/nosana/frpc:0.1.0\u001b[22m\u001b[39m"]
[12.691219, "o", "\u001b[1G\u001b[0K"]
[12.691227, "o", "\u001b[?25h"]
[12.691258, "o", "\u001b[32m✔\u001b[39m \u001b[32mRunning container \u001b[1mregistry.hub.docker.com/nosana/frpc:0.1.0\u001b[22m\u001b[39m\r\n"]
[12.691708, "o", "\u001b[?25l"]
[12.691734, "o", "\u001b[1G\u001b[1G\u001b[36m⠋\u001b[39m \u001b[36mStarting container \u001b[1mdocker.io/vllm/vllm-openai:v0.7.2\u001b[22m\u001b[39m"]
[12.691757, "o", "\u001b[1G\u001b[0K"]
[12.691763, "o", "\u001b[?25h"]
[12.691772, "o", "\u001b[32m✔\u001b[39m \u001b[32mRunning container \u001b[1mdocker.io/vllm/vllm-openai:v0.7.2\u001b[22m\u001b[39m\r\n"]
[12.691785, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000KINFO 03-11 04:29:13 __init__.py:190] Automatically detected platform cuda.\r\n"]
[12.691794, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000EINFO 03-11 04:29:15 api_server.py:840] vLLM API server version 0.7.2\r\n"]
[12.691809, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\r|INFO 03-11 04:29:15 api_server.py:841] args: Namespace(host=None, port=9000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=130000, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lo"]
[12.691816, "o", "ra_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['R1-Qwen-1.5B'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)\r\n"]
[12.691822, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000JINFO 03-11 04:29:15 api_server.py:206] Started engine process with PID 30\r\n"]
[12.691826, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000KINFO 03-11 04:29:19 __init__.py:190] Automatically detected platform cuda.\r\n"]
[12.691845, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000�INFO 03-11 04:29:23 config.py:542] This model supports multiple tasks: {'reward', 'generate', 'score', 'embed', 'classify'}. Defaulting to 'generate'.\r\n"]
[12.691852, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0001$WARNING 03-11 04:29:23 arg_utils.py:1135] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\r\n"]
[12.691858, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000aINFO 03-11 04:29:23 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=2048.\r\n"]
[12.691867, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000�INFO 03-11 04:29:27 config.py:542] This model supports multiple tasks: {'classify', 'generate', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\r\n"]
[12.691873, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0001$WARNING 03-11 04:29:27 arg_utils.py:1135] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\r\n"]
[12.691879, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000aINFO 03-11 04:29:27 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=2048.\r\n"]
[12.691891, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0005BINFO 03-11 04:29:27 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=130000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=R1-Qwen-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \r\n"]
[12.691893, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000zWARNING 03-11 04:29:29 interface.py:284] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\r\n"]
[12.691903, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000@INFO 03-11 04:29:29 cuda.py:230] Using Flash Attention backend.\r\n"]
[12.693822, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000nINFO 03-11 04:29:30 model_runner.py:1110] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\r\n"]
[12.693833, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000VINFO 03-11 04:29:30 weight_utils.py:252] Using model weights format ['*.safetensors']\r\n\u0001\u0000\u0000\u0000\u0000\u0000\u0000ZINFO 03-11 04:35:25 weight_utils.py:297] No model.safetensors.index.json found in remote.\r\n"]
[12.693842, "o", "\u0002\u0000\u0000\u0000\u0000\u0000\u0000N\rLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n"]
[12.693848, "o", "\u0002\u0000\u0000\u0000\u0000\u0000\u0000V\rLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.18s/it]\r\n"]
[12.693854, "o", "\u0002\u0000\u0000\u0000\u0000\u0000\u0000V\rLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.18s/it]\r\n"]
[12.693862, "o", "\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0001\r\n"]
[12.69387, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000OINFO 03-11 04:35:26 model_runner.py:1115] Loading model weights took 3.3460 GB\r\n"]
[12.693879, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000HINFO 03-11 04:35:27 worker.py:267] Memory profiling takes 0.65 seconds\r\r\n"]
[12.69389, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000�INFO 03-11 04:35:27 worker.py:267] the current vLLM instance can use total_gpu_memory (24.00GiB) x gpu_memory_utilization (0.90) = 21.60GiB\r\r\n"]
[12.693905, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000�INFO 03-11 04:35:27 worker.py:267] model weights take 3.35GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 16.81GiB.\r\n"]
[12.693916, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000SINFO 03-11 04:35:27 executor_base.py:110] # CUDA blocks: 39334, # CPU blocks: 9362\r\n"]
[12.693924, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000cINFO 03-11 04:35:27 executor_base.py:115] Maximum concurrency for 130000 tokens per request: 4.84x\r\n"]
[12.693939, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0001�INFO 03-11 04:35:28 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n"]
[12.693968, "o", "\u0002\u0000\u0000\u0000\u0000\u0000\f�\rCapturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\rCapturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:13,  2.55it/s]\rCapturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:12,  2.65it/s]\rCapturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:11,  2.76it/s]\rCapturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:11,  2.78it/s]\rCapturing CUDA graph shapes:  14%|█▍        | 5/35 [00:01<00:10,  2.74it/s]\rCapturing CUDA graph shapes:  17%|█▋        | 6/35 [00:02<00:10,  2.73it/s]\rCapturing CUDA graph shapes:  20%|██        | 7/35 [00:02<00:10,  2.61it/s]\rCapturing CUDA graph shapes:  23%|██▎       | 8/35 [00:02<00:10,  2.63it/s]\rCapturing CUDA graph shapes:  26%|██▌       | 9/35 [00:03<00:10,  2.59it/s]\rCapturing CUDA graph shapes:  29%|██▊       | 10/35 [00:03<00:09,  2.60it/s]\rCapturing CUDA graph shapes:  31%|███▏      | 11/35 [00:04<00:09,  2.62it/s]\rCapturing CUDA graph shapes:  34%|███▍      | 12/35 [00:04<00:09,  2.46it/s]\rCapturing CUDA graph shapes:  37%|███▋      | 13/35 [00:04<00:08,  2.56it/s]\rCapturing CUDA graph shapes:  40%|████      | 14/35 [00:05<00:07,  2.66it/s]\rCapturing CUDA graph shapes:  43%|████▎     | 15/35 [00:05<00:07,  2.73it/s]\rCapturing CUDA graph shapes:  46%|████▌     | 16/35 [00:06<00:06,  2.76it/s]\rCapturing CUDA graph shapes:  49%|████▊     | 17/35 [00:06<00:06,  2.75it/s]\rCapturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:06<00:06,  2.75it/s]\rCapturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:07<00:05,  2.74it/s]\rCapturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:07<00:05,  2.72it/s]\rCapturing CUDA graph shapes:  60%|██████    | 21/35 [00:07<00:05,  2.57it/s]\rCapturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:08<00:05,  2.43it/s]\rCapturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:08<00:04,  2.54it/s]\rCapturing CUDA graph shapes:  6"]
[12.693986, "o", "9%|██████▊   | 24/35 [00:09<00:04,  2.63it/s]\rCapturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:09<00:03,  2.56it/s]\rCapturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:09<00:03,  2.44it/s]\rCapturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:10<00:03,  2.31it/s]\rCapturing CUDA graph shapes:  80%|████████  | 28/35 [00:10<00:03,  2.25it/s]\rCapturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:11<00:02,  2.27it/s]\rCapturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:11<00:02,  2.44it/s]\rCapturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:12<00:01,  2.52it/s]\rCapturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:12<00:01,  2.52it/s]\rCapturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:12<00:00,  2.64it/s]\rCapturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:13<00:00,  2.65it/s]\rCapturing CUDA graph shapes: 100%|██████████| 35/35 [00:13<00:00,  2.67it/s]\rCapturing CUDA graph shapes: 100%|██████████| 35/35 [00:13<00:00,  2.59it/s]\r\n"]
[12.693988, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000]INFO 03-11 04:35:42 model_runner.py:1562] Graph capturing finished in 14 secs, took 0.16 GiB\r\n"]
[12.694002, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000oINFO 03-11 04:35:42 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 15.43 seconds\r\n"]
[12.694011, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000FINFO 03-11 04:35:42 api_server.py:756] Using supplied chat template:\r\r\n"]
[12.694019, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000,INFO 03-11 04:35:42 api_server.py:756] None\r\n"]
[12.694032, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000:INFO 03-11 04:35:42 launcher.py:21] Available routes are:\r\n"]
[12.694041, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000MINFO 03-11 04:35:42 launcher.py:29] Route: /openapi.json, Methods: GET, HEAD\r\n"]
[12.694053, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000EINFO 03-11 04:35:42 launcher.py:29] Route: /docs, Methods: GET, HEAD\r\n"]
[12.694061, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000UINFO 03-11 04:35:42 launcher.py:29] Route: /docs/oauth2-redirect, Methods: GET, HEAD\r\n"]
[12.694069, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000FINFO 03-11 04:35:42 launcher.py:29] Route: /redoc, Methods: GET, HEAD\r\n"]
[12.694079, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000AINFO 03-11 04:35:42 launcher.py:29] Route: /health, Methods: GET\r\n"]
[12.694089, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000EINFO 03-11 04:35:42 launcher.py:29] Route: /ping, Methods: GET, POST\r\n"]
[12.694096, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000DINFO 03-11 04:35:42 launcher.py:29] Route: /tokenize, Methods: POST\r\n"]
[12.694105, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000FINFO 03-11 04:35:42 launcher.py:29] Route: /detokenize, Methods: POST\r\n"]
[12.694114, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000DINFO 03-11 04:35:42 launcher.py:29] Route: /v1/models, Methods: GET\r\n"]
[12.694123, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000BINFO 03-11 04:35:42 launcher.py:29] Route: /version, Methods: GET\r\n"]
[12.694132, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000OINFO 03-11 04:35:42 launcher.py:29] Route: /v1/chat/completions, Methods: POST\r\n"]
[12.694141, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000JINFO 03-11 04:35:42 launcher.py:29] Route: /v1/completions, Methods: POST\r\n"]
[12.69415, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000IINFO 03-11 04:35:42 launcher.py:29] Route: /v1/embeddings, Methods: POST\r\n"]
[12.694162, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000CINFO 03-11 04:35:42 launcher.py:29] Route: /pooling, Methods: POST\r\n"]
[12.69417, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000AINFO 03-11 04:35:42 launcher.py:29] Route: /score, Methods: POST\r\n"]
[12.694177, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000DINFO 03-11 04:35:42 launcher.py:29] Route: /v1/score, Methods: POST\r\n"]
[12.694185, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000BINFO 03-11 04:35:42 launcher.py:29] Route: /rerank, Methods: POST\r\n"]
[12.694196, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000EINFO 03-11 04:35:42 launcher.py:29] Route: /v1/rerank, Methods: POST\r\n"]
[12.694204, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000EINFO 03-11 04:35:42 launcher.py:29] Route: /v2/rerank, Methods: POST\r\n"]
[12.694218, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000GINFO 03-11 04:35:42 launcher.py:29] Route: /invocations, Methods: POST\r\n"]
[12.85165, "o", "\u0002\u0000\u0000\u0000\u0000\u0000\u0000&INFO:     Started server process [14]\r\n\u0002\u0000\u0000\u0000\u0000\u0000\u0000+INFO:     Waiting for application startup.\r\n\u0002\u0000\u0000\u0000\u0000\u0000\u0000(INFO:     Application startup complete.\r\n\u0002\u0000\u0000\u0000\u0000\u0000\u0000HINFO:     Uvicorn running on http://0.0.0.0:9000 (Press CTRL+C to quit)\r\n"]
[12.851688, "o", "\u001b[32mJob \u001b[1mB7owLMjUup9znGX6BHVoUm2fwo2dgt8frG6dTE6pMU9y\u001b[22m is now exposed (\u001b[1mhttps://B7owLMjUup9znGX6BHVoUm2fwo2dgt8frG6dTE6pMU9y.node.k8s.prd.nos.ci\u001b[22m)\u001b[39m\r\n"]
[12.851768, "o", "\u0001\u0000\u0000\u0000\u0000\u0000\u0000;INFO:     127.0.0.1:37132 - \"GET / HTTP/1.1\" 404 Not Found\r\n"]
[21.224194, "o", "\r\n"]
[23.038917, "o", "^C"]
[23.039519, "o", "\u001b[?25h"]
[23.053253, "o", "\r\n"]
[23.059255, "o", "\u001b[?2004h\u001b]0;nosana@nos-os: ~\u0007\u001b[01;32mnosana@nos-os\u001b[00m:\u001b[01;34m~\u001b[00m$ "]
[23.59067, "o", "\u001b[?2004l\r\r\nexit\r\n"]
